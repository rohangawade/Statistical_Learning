---
title: "Model Selection - Best Subset, Forward Stepwise selection"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r cars}
library(ISLR)
```
This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.
```{r}
summary(Hitters)
```
We will predict salary for the players.
But we have missing values here. In this case we will
remove missing values. We can also impute the data with mean or median or mode depending on the distribution.
```{r pressure, echo=FALSE}
with(Hitters,sum(is.na(Salary)))
hitters = na.omit(Hitters)
with(hitters,sum(is.na(Salary)))
```
Best Subset Regression
Here we go through all models of different subset sizes and looks for best in each size. 2^p models where p is number of predictors.
Library 'leaps' can be used here
```{r}
library(leaps)
```

```{r}
regfit.full=regsubsets(Salary~.,data= hitters)
summary(regfit.full)
```

* indicates feature selected for that particular siz.
by default we get for size 8
```{r}
#for all predictors.
regfit.full1=regsubsets(Salary~.,data= hitters,nvmax = 19)
reg.summary = summary(regfit.full1)
names(reg.summary)
```
```{r}
mincp= which.min(reg.summary$cp)
plot(reg.summary$cp,xlab="Number of variables",ylab="cp")

points(mincp,reg.summary$cp[mincp],pch=20,col="red")
```
THe lowest prediction error is for 10 variable.

Using plot method for 'regsubsets' object
```{r}
plot(regfit.full1,scale="Cp")
```
The black one indicates In features and white indicates out features
```{r}
coef(regfit.full1,mincp)
```
Coefficients for 10 variables in the model.
-----------------------------------------------
Forward Stepwise selection - greedy algorithm. 
Each time it includes next best variable, but it produces in nested sequence
```{r}
regfit.fwd = regsubsets(Salary~.,data = hitters,nvmax = 19,method = "forward")
```

```{r}
summary(regfit.fwd)
```
Each model contains all variables from previous model plus 1 new variable.
```{r}
plot(regfit.fwd,scale="Cp")
```
Similar structure like best subset.
----------------------------------
Model Selection using validation set
```{r}
dim(hitters)
```

```{r}
set.seed(1)
#2/3 train and 1/3 test
#sample from 263, 180 observations
train=sample(seq(263),180,replace=FALSE)
```

```{r}
regfit.fwdvs = regsubsets(Salary~.,data = hitters[train,],nvmax = 19,method = "forward")
```
Predictions on validation set 
Since we will have 19 models we use a vector to keep track of error.
```{r}
val.errors = rep(NA,19)
```

```{r}
x.test= model.matrix(Salary~.,data=hitters[-train,])
```
#no predict method for regsubset
```{r}
for(i in 1:19){
  coefi=coef(regfit.fwdvs,id = i)
  pred=x.test[,names(coefi)]%*%coefi #matrix multiply with coeffi vector
  val.errors[i]= mean((hitters$Salary[-train]-pred)^2)
}
```

```{r}
plot(sqrt(val.errors),ylab = "Root MSE",ylim=c(300,400),pch=19,type="b")
#-1 removes the null model
points(sqrt(regfit.fwdvs$rss[-1]/180),col="blue",pch=19,type="b")
legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)

```
The rss decreases as each time a new variable is included the fit of the model is improved on training data

Since we dont have predict method for regsubsets. Lets write a function for it
```{r}
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  testmat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  testmat[,names(coefi)]%*%coefi
}
```
