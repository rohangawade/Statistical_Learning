---
title: "Model Selection - Best Subset, Forward Stepwise selection using validation Set, Cross Validation"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r cars}
library(ISLR)
```
This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.
```{r}
summary(Hitters)
```
We will predict salary for the players.
But we have missing values here. In this case we will
remove missing values. We can also impute the data with mean or median or mode depending on the distribution.
```{r pressure, echo=FALSE}
with(Hitters,sum(is.na(Salary)))
hitters = na.omit(Hitters)
with(hitters,sum(is.na(Salary)))
```
Best Subset Regression
Here we go through all models of different subset sizes and looks for best in each size. 2^p models where p is number of predictors.
Library 'leaps' can be used here
```{r}
library(leaps)
```

```{r}
regfit.full=regsubsets(Salary~.,data= hitters)
summary(regfit.full)
```

* indicates feature selected for that particular siz.
by default we get for size 8
```{r}
#for all predictors.
regfit.full1=regsubsets(Salary~.,data= hitters,nvmax = 19)
reg.summary = summary(regfit.full1)
names(reg.summary)
```
```{r}
mincp= which.min(reg.summary$cp)
plot(reg.summary$cp,xlab="Number of variables",ylab="cp")

points(mincp,reg.summary$cp[mincp],pch=20,col="red")
```
THe lowest prediction error is for 10 variable.

Using plot method for 'regsubsets' object
```{r}
plot(regfit.full1,scale="Cp")
```
The black one indicates In features and white indicates out features
```{r}
coef(regfit.full1,mincp)
```
Coefficients for 10 variables in the model.
-----------------------------------------------
Forward Stepwise selection - greedy algorithm. 
Each time it includes next best variable, but it produces in nested sequence
```{r}
regfit.fwd = regsubsets(Salary~.,data = hitters,nvmax = 19,method = "forward")
```

```{r}
summary(regfit.fwd)
```
Each model contains all variables from previous model plus 1 new variable.
```{r}
plot(regfit.fwd,scale="Cp")
```
Similar structure like best subset.
----------------------------------
Model Selection using validation set
```{r}
dim(hitters)
```

```{r}
set.seed(1)
#2/3 train and 1/3 test
#sample from 263, 180 observations
train=sample(seq(263),180,replace=FALSE)
```

```{r}
regfit.fwdvs = regsubsets(Salary~.,data = hitters[train,],nvmax = 19,method = "forward")
```
Predictions on validation set 
Since we will have 19 models we use a vector to keep track of error.
```{r}
val.errors = rep(NA,19)
```

```{r}
x.test= model.matrix(Salary~.,data=hitters[-train,])
```
#no predict method for regsubset
```{r}
for(i in 1:19){
  coefi=coef(regfit.fwdvs,id = i)
  pred=x.test[,names(coefi)]%*%coefi #matrix multiply with coeffi vector
  val.errors[i]= mean((hitters$Salary[-train]-pred)^2)
}
```

```{r}
plot(sqrt(val.errors),ylab = "Root MSE",ylim=c(300,400),pch=19,type="b")
#-1 removes the null model
points(sqrt(regfit.fwdvs$rss[-1]/180),col="blue",pch=19,type="b")
legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)

```
The rss decreases as each time a new variable is included the fit of the model is improved on training data

Since we dont have predict method for regsubsets. Lets write a function for it
```{r}
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  testmat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  testmat[,names(coefi)]%*%coefi
}
```

-----------------------
Model Selection by Cross-validation - 10 fold
```{r}
set.seed(11)
folds=sample(rep(1:10,length = nrow(hitters)))
folds
```
random assignment of folds to each observation in hitters
```{r}
table(folds)
```
Balanced assignment of folds.
```{r}
#10 rows for each of the 10 fold and 19 columns because there are 19 variables so 19 subsets.
cv.errors = matrix(NA,10,19)
```

```{r}
for(k in 1:10){
  best.fit=regsubsets(Salary~.,data=hitters[folds!=k,],nvmax=19,method="forward")
  for(i in 1:19){
    #as our predict.regsubsets function is written in a way generic predict understands, we can just use predict()
    pred=predict(best.fit,hitters[folds==k,],id=i)
    cv.errors[k,i]=mean((hitters$Salary[folds==k]-pred)^2)
  }  
}
```

taking column means as each row was mean squared error for a particular fold and then take root of it to get rmse for model with that number of variables.
```{r}
rmse.cv=sqrt(apply(cv.errors,2,mean))
plot(rmse.cv,pch=19, type="b")
```

This erros are computed using full training set, done fold by fold and averaged.
This seems to favor model size of 11 or 12.

--------------------------------------------------
Ridge Regression and Lasso Regression
```{r}
library(glmnet)
```
We need to provide matrix x of predictors and vector y to glmnet as it doesnot use formula
```{r}
X=model.matrix(Salary~.,-1,data=hitters)
y=hitters$Salary
```
Fitting a ridge regression model.
using glmnet with alpha=0 for ridge (l2 norm)
alpha=1 for lasso (l1 norm)
```{r}
fit.ridge = glmnet(X,y,alpha=0)
```

```{r}
plot(fit.ridge,xvar="lambda",label=TRUE)
```
It makes plot as a function of log of lambda with the co-efficients. lambda controls the penalty on coeffiecients where ridge regression is penalised by sum of squares of coefficients.
As the lambda gets big the sum of squares of co-efficients shrinks to zero. 
So as we see on log scale when lambda =12 we have all the coeffiecients = 0, and as we relax lambda the co-efficient increase until lambda is 0 and coefficients are unregularized.
Ridge Regression keeps all the variables in and shrink coefficients to zero where as best and forward stepwise selection controls the model complexity by restricting number of variables.
```{r}
#cross validation using glmnet, by default k =10
cv.ridge=cv.glmnet(X,y,alpha=0)
plot(cv.ridge)
```
The first line on the left indicates minimum error while other on right is at one standard error of minimum. 
As we can see there are all variables in the model.(19 variables)

--------------------------------------
Lasso - alpha =1 by default
```{r}
fit.lasso =glmnet(X,y)
```

```{r}
plot(fit.lasso,xvar="lambda",label=TRUE)
```
Initially all coeffiencts are zero. THen the first coefficient is the purple line and then green one jumps in and then red.
At top of the plot we can see the number of variables in model as function of lambda.
Lasso does variable selection and shrinkage of coeffiencts
```{r}
#Deviance, percentage of deviance explained or in case of regression, percentage of sum of squares explained, i.e r squared.
plot(fit.lasso,xvar="dev",label=TRUE)
```
Now the orientation is different compare to lambda
Fraction of Deviance explained is equivalent to r sqaured.
As we can see a lot of r squared is explained for heavy shrinkage of coefficients. While after 0.4 or 0.5
towards the end the relatively small increase in r squared, co-efficients grow large. which may indicate overfitting at the end of the path.
```{r}
cv.lasso = cv.glmnet(X,y)
plot(cv.lasso)
```
The minimum cross validation error is at model of size 15 and withing one standard error is at model of size 6 or 5
```{r}
coef(cv.lasso)
```
Coefficient vector wrt best model which is within one standar error of mininmum. 
As we are using cv.glmnet if we use model by fit.glmnet we will have roughtly 100 of vectors depending on each value, indexed by different lambda values.
-------------------------------------
using train -validation division.
```{r}
lasso.train = glmnet(X[train,],y[train])
lasso.train
```
Degree of freedom, number of variables. it tries to predict for 100 values of lambda but we can see, there is no change in percentage deviation as lambda gets smaller, so it stops.
```{r}
predlasso = predict(lasso.train,X[-train,])
dim(predlasso)
```
83 observations in validation set and 89 values of lambda.
```{r}
#here y[-train] is a vector of 83 and pred is 83*89, here R recycles vector 89 times and column wise and gives result of matrix 83*89
rmse=sqrt(apply((y[-train]-predlasso)^2,2,mean))
plot(log(lasso.train$lambda),rmse,type="b",xlab="log(lambda)")
```
log(lambda) approx < 2 seems to be overfitting, > 4 underfitting while around 3 seems to be ideal value. 
```{r}
#get lambda value for low rmse. we index the lambdas with order of rmse.
lambda.best=lasso.train$lambda[order(rmse)[1]]
lambda.best
```

```{r}
coef(lasso.train,s=lambda.best)
```
This features can be used to build a model.
cv.glmnet gave us 5 variables.
