---
title: "Resampling Method Notebook - Exercise solution"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---


```{r}
load("GitHub/Statistical_Learning/ResamplingMethods/5.R.RData")
```

```{r}
names(Xy)
```
1. Consider the linear regression model of y on X1 and X2. What is the standard error for beta1?
```{r}
m1 = lm(y~.,Xy)
```


```{r}
summary(m1)
```
beta1 is co-efficient fo X1. The se is 0.02593

```{r}
matplot(Xy,type="l")
legend("bottomleft", inset=0.01, legend=c("X1","X2","y"), col=c(1:3),pch=4,
bg= ("white"), horiz=F)
```
There is very strong autocorrelation between consecutive rows of the data matrix. Roughly speaking, we have about 10-20 repeats of every data point, so the sample size is in effect much smaller than the number of rows (1000 in this case).

3. Now, use the (standard) bootstrap to estimate se . To within 10%, what do you get?
```{r}
alpha=function(x,y){
  cx=var(x)
  cy=var(y)
  cxy=cov(x,y)
  (cy-cxy)/(cx+cy-2*cxy)
}
alpha.fn1<-function(data, index) {
  fit1<-lm(y~., data=Xy[index,])
  coefficients(fit1)[['X1']]
}
```

```{r}
set.seed(1)
```

```{r}

boot.out=boot::boot(Xy,alpha.fn1,R=1000)
boot.out
```
The standard error is 0.0287

4. Finally, use the block bootstrap to estimate se. Use blocks of 100 contiguous observations, and resample ten whole blocks with replacement then paste them together to construct each bootstrap time series. For example, one of your bootstrap resamples could be:

new.rows = c(101:200, 401:500, 101:200, 901:1000, 301:400, 1:100, 1:100, 801:900, 201:300, 701:800)

new.Xy = Xy[new.rows, ]

To within 10%, what do you get?
```{r}
new_index=c(1,101,201,301,401,501,601,701,801,901)
se = c()

for (i in 1:1000){
    newrows = c()
    new_start = sample(new_index,10,replace = TRUE)
    
    for (j in new_start){
        newrows = c(newrows,seq(j,j+99))
        }
    fit = lm(y~X1+X2, data = Xy[newrows,])
    
    seb1 = c(save,fit$coefficients[2])

}

sd(seb1)
```


